{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdfdf3",
   "metadata": {
    "id": "debdfdf3"
   },
   "outputs": [],
   "source": [
    "def tokenize(df_col):\n",
    "    all_tokenized = []\n",
    "    for i in range(len(df_col)):\n",
    "        word_punct_token = WordPunctTokenizer().tokenize(data_preprocessed.iloc[i])\n",
    "        clean_token=[]\n",
    "        for token in word_punct_token:\n",
    "            token = token.lower()\n",
    "            # remove any value that are not alphabetical\n",
    "            new_token = re.sub(r'[^a-zA-Z]+', '', token) \n",
    "            # remove empty value and single character value\n",
    "            if new_token != \"\" and len(new_token) >= 2: \n",
    "                vowels=len([v for v in new_token if v in \"aeiou\"])\n",
    "                if vowels != 0: # remove line that only contains consonants\n",
    "                    clean_token.append(new_token)\n",
    "        all_tokenized.append(clean_token)\n",
    "    \n",
    "    return all_tokenized\n",
    "\n",
    "def remove_stop_words(arr):\n",
    "    # Get the list of stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    # add new stopwords to the list\n",
    "    stop_words.extend([\"could\",\"though\",\"would\",\"also\",\"many\",'much'])\n",
    "    # print(stop_words)\n",
    "    # Remove the stopwords from the list of tokens\n",
    "    tokens = [x for x in arr if x not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def token_to_text(tokens):\n",
    "    text = \"\"\n",
    "    for word in tokens:\n",
    "        text = text + \" \" + word\n",
    "    \n",
    "    return text\n",
    "\n",
    "def count_probabilities(text, length):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vector = vectorizer.fit_transform([text])\n",
    "    vector = vector.toarray()\n",
    "    vector = vector[0]/length\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    result_dictionary = dict(zip(vocab, vector))\n",
    "    return result_dictionary\n",
    "\n",
    "def get_global_vocab(all_tokens):\n",
    "    global_vocab = []\n",
    "    for row in all_tokens:\n",
    "        for word in row:\n",
    "            global_vocab.append(word)\n",
    "    return global_vocab\n",
    "\n",
    "def get_unique_global_vocab(all_tokens):\n",
    "    global_vocab = []\n",
    "    for row in all_tokens:\n",
    "        for word in row:\n",
    "            global_vocab.append(word)\n",
    "    unique_vocab = list(set(global_vocab))\n",
    "    \n",
    "    return unique_vocab\n",
    "\n",
    "def get_global_dictionary(global_vocab):\n",
    "    dic = {}\n",
    "    for word in global_vocab:\n",
    "        if word in dic:\n",
    "            dic[word] += 1\n",
    "        else:\n",
    "            dic[word] = 0\n",
    "    return dic\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad2a9d2",
   "metadata": {
    "id": "cad2a9d2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import html2text\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import textcleaner as tc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a65020",
   "metadata": {
    "id": "13a65020"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"untag.csv\")\n",
    "test_df = pd.read_csv(\"untag_test.csv\")\n",
    "train_df = train_df.sample(frac=1, axis=1).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefef2b3",
   "metadata": {
    "id": "cefef2b3"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train_df,test_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d571ede",
   "metadata": {
    "id": "5d571ede"
   },
   "outputs": [],
   "source": [
    "train_df.Source.value_counts();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad825414",
   "metadata": {},
   "source": [
    "###  Balancing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d8b26",
   "metadata": {
    "id": "475d8b26"
   },
   "outputs": [],
   "source": [
    "df[df.Source == \"Businessweek\"]\n",
    "df = df.drop(1115)\n",
    "df = df.drop(804)\n",
    "df = df.drop(1433)\n",
    "df = df.drop(1949)\n",
    "df = df.drop(2445)\n",
    "df = df.drop(1925)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80088ad",
   "metadata": {
    "id": "b80088ad"
   },
   "outputs": [],
   "source": [
    "df[df.Source == \"Contactmusic.com\"]\n",
    "df = df.drop(99)\n",
    "df = df.drop(400)\n",
    "df = df.drop(1192)\n",
    "df = df.drop(1774)\n",
    "df = df.drop(2079)\n",
    "df = df.drop(994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebba3c",
   "metadata": {
    "id": "b2ebba3c"
   },
   "outputs": [],
   "source": [
    "df[df.Source == \"Forbes\"]\n",
    "df = df.drop(949)\n",
    "df = df.drop(963)\n",
    "df = df.drop(1016)\n",
    "df = df.drop(1093)\n",
    "df = df.drop(2680)\n",
    "df = df.drop(1466)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18893e60",
   "metadata": {
    "id": "18893e60"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692f9d1",
   "metadata": {
    "id": "1692f9d1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39028f13",
   "metadata": {
    "id": "39028f13"
   },
   "outputs": [],
   "source": [
    "data_preprocessed = df[\"Tag Removed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b5246",
   "metadata": {
    "id": "602b5246"
   },
   "outputs": [],
   "source": [
    "tokenized = tokenize(data_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9ef39",
   "metadata": {
    "id": "e5d9ef39"
   },
   "outputs": [],
   "source": [
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75159db2",
   "metadata": {
    "id": "75159db2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a5cda5",
   "metadata": {
    "id": "99a5cda5"
   },
   "outputs": [],
   "source": [
    "stop_words_removed = []\n",
    "for arr in tokenized:\n",
    "    stop_words_removed.append(remove_stop_words(arr))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b831e7",
   "metadata": {
    "id": "90b831e7"
   },
   "outputs": [],
   "source": [
    "unique_global_vocab = get_unique_global_vocab(stop_words_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5636e2e7",
   "metadata": {
    "id": "5636e2e7"
   },
   "outputs": [],
   "source": [
    "global_dict = {}\n",
    "temp = get_global_vocab(stop_words_removed)\n",
    "\n",
    "x = get_global_dictionary(temp)\n",
    "\n",
    "all_words = dict(sorted(x.items(), key=lambda item: item[1],reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de84745",
   "metadata": {
    "id": "1de84745"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e4b2b2",
   "metadata": {
    "id": "76e4b2b2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for key,value in all_words.items():\n",
    "#     if value > 0 and value < 2:\n",
    "#         print(key,end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b0a7c",
   "metadata": {
    "id": "c60b0a7c"
   },
   "outputs": [],
   "source": [
    "final_dict = {}\n",
    "final_vocab = []\n",
    "\n",
    "for key,value in all_words.items():\n",
    "    if value < 3000 and value > 5:\n",
    "        if key not in final_dict:\n",
    "            final_dict[key] = value\n",
    "            final_vocab.append(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a1402",
   "metadata": {
    "id": "3b5a1402"
   },
   "outputs": [],
   "source": [
    "final_stop_words_removed = []\n",
    "\n",
    "for row in stop_words_removed:\n",
    "    temp = []\n",
    "    for word in row:\n",
    "        if word in final_dict:\n",
    "            temp.append(word)\n",
    "    final_stop_words_removed.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcba1f1",
   "metadata": {
    "id": "1fcba1f1"
   },
   "outputs": [],
   "source": [
    "zero_arr = np.zeros((len(df),len(final_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82f6af",
   "metadata": {
    "id": "4d82f6af"
   },
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(data=zero_arr, columns=final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a17dd6",
   "metadata": {
    "id": "93a17dd6",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b745a",
   "metadata": {
    "id": "410b745a"
   },
   "outputs": [],
   "source": [
    "text_array = []\n",
    "\n",
    "for row in final_stop_words_removed:\n",
    "    text_array.append(token_to_text(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4459b",
   "metadata": {
    "id": "6fd4459b",
    "outputId": "9d0eb871-f8c4-44d5-a410-232525ed476e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200->401->602->803->1004->1205->1406->1607->1808->2009->2210->2411->2612->2813->3014->3215->3416->"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for index in range(len(df)):\n",
    "    if(len(final_stop_words_removed[index])>0):\n",
    "        word_probabilities = count_probabilities(text_array[index],len(final_stop_words_removed[index]))\n",
    "        for key,value in word_probabilities.items():\n",
    "            word_df.loc[index,key] = word_probabilities[key]\n",
    "    count += 1\n",
    "    if count > 200:\n",
    "        print(index,end=\"->\")\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78577dc",
   "metadata": {
    "id": "e78577dc"
   },
   "outputs": [],
   "source": [
    "df.head(2732)\n",
    "num = 2705"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca191b",
   "metadata": {
    "id": "bbca191b"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e9c5fd",
   "metadata": {
    "id": "18e9c5fd"
   },
   "outputs": [],
   "source": [
    "X = word_df.head(num)\n",
    "y = df[\"Source\"].head(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb9345",
   "metadata": {
    "id": "fbeb9345"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9182cb6b",
   "metadata": {
    "id": "9182cb6b"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200,oob_score=True, n_jobs = 3,\n",
    "                               bootstrap=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f8474",
   "metadata": {
    "id": "529f8474",
    "outputId": "4234ab08-ae1c-4a34-db1b-baca82d4a2dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, n_jobs=3, oob_score=True,\n",
       "                       random_state=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bcf83f",
   "metadata": {
    "id": "97bcf83f"
   },
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b2a727",
   "metadata": {
    "id": "36b2a727"
   },
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e92bc",
   "metadata": {
    "id": "fc3e92bc"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(word_df.tail(688))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ece57d",
   "metadata": {
    "id": "d5ece57d"
   },
   "outputs": [],
   "source": [
    "ids = list(range(1,689))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66f58d",
   "metadata": {
    "id": "6a66f58d"
   },
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a4dbe",
   "metadata": {
    "id": "6c7a4dbe"
   },
   "outputs": [],
   "source": [
    "new_df[\"Id\"] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923bf768",
   "metadata": {
    "id": "923bf768"
   },
   "outputs": [],
   "source": [
    "new_df[\"Publisher\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba969b7",
   "metadata": {
    "id": "5ba969b7"
   },
   "outputs": [],
   "source": [
    "new_df.to_csv(\"results_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3042bf",
   "metadata": {
    "id": "4f3042bf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd1b9cb2",
   "metadata": {
    "id": "bd1b9cb2"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "AIC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
